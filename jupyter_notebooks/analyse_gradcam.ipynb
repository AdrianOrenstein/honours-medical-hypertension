{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.experiments import get_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment = get_experiment('resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, precision=16)\n",
    "\n",
    "experiment = experiment.load_from_checkpoint(\n",
    "    '../.data/resnet50_k_v.ckpt',\n",
    "    train_data_path='../.data/medical_data_train',\n",
    "    val_data_path='../.data/medical_data_val',\n",
    "    learning_rate= 0.003,\n",
    "    batch_size= 64,\n",
    "    sequence_length= 16384,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from grad_cam import *\n",
    "import os.path as osp\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "from dataclasses import dataclass\n",
    "from src.datasets.medical_data import PadData\n",
    "def save_gradcam(filename, gcam, raw_image, paper_cmap=False):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet_r(gcam)\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n",
    "    \n",
    "    #cv2.imwrite(filename, np.uint8(gcam))\n",
    "    \n",
    "    return np.uint8(gcam), cmap\n",
    "\n",
    "def colorize(words, color_array):\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for word, color in zip(words, color_array):\n",
    "        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n",
    "    return colored_string\n",
    "\n",
    "def get_colors(inp, colormap, vmin=None, vmax=None):\n",
    "    norm = plt.Normalize(vmin, vmax)\n",
    "    return colormap(norm(inp))\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    target_layer: str\n",
    "    words: list\n",
    "    sensitivity: torch.Tensor\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    results: List[Sample] = []\n",
    "\n",
    "    # Synset words\n",
    "    classes = [0, 1]\n",
    "\n",
    "    # Model\n",
    "    model = experiment.model\n",
    "\n",
    "\n",
    "    # The four residual layers\n",
    "    target_layers = [\"features.6.0.residual_block\", \"features.7.0.residual_block\", \"features.8.0.residual_block\",\"features.9.0.residual_block\"]\n",
    "    target_class = 0\n",
    "\n",
    "    output_dir = '.'\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        experiment.data_module.val_dataset,\n",
    "        batch_size=experiment.batch_size,\n",
    "        collate_fn=PadData(\n",
    "            pad_to_length=experiment.data_module.sequence_length,\n",
    "            pad_val=experiment.data_module.pad_token_id,\n",
    "        ),\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    gcam = GradCAM(model=model)\n",
    "    \n",
    "    i = 0\n",
    "    for _, batch in tqdm(enumerate(test_dataloader)):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        x, y, y_hat, loss = experiment.step(batch, \"eval\")\n",
    "        gcam.device = x.device\n",
    "        y_hat = torch.softmax(y_hat, dim=1)\n",
    "        \n",
    "        gcam.logits = y_hat\n",
    "        gcam.image_shape = x.shape\n",
    "        \n",
    "        confidences, predicted = y_hat.max(1)\n",
    "        \n",
    "        mask = (y == 1) & (predicted == 1)\n",
    "\n",
    "        # only look at correct predictions\n",
    "        x = x[mask]\n",
    "        predicted = predicted[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "\n",
    "        ids_ = torch.LongTensor([[target_class]] * len(x))\n",
    "        gcam.backward(ids=ids_)\n",
    "\n",
    "\n",
    "        for target_layer in target_layers:\n",
    "#             print(\"Generating Grad-CAM @{}\".format(target_layer))\n",
    "\n",
    "            # Grad-CAM\n",
    "            regions = gcam.generate(target_layer=target_layer).squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "            for j, sample in enumerate(x):            \n",
    "                words = experiment.model.tokeniser.convert_ids_to_tokens(x[j, :].tolist())\n",
    "\n",
    "                # cmap\n",
    "                cm = plt.get_cmap('plasma', lut=8)\n",
    "\n",
    "                color_array = [matplotlib.colors.to_hex(color) for color in get_colors(regions[j, :], cm, 0, 1)]\n",
    "\n",
    "                s = colorize(words, color_array)\n",
    "\n",
    "#                     # or simply save in an html file and open in browser\n",
    "#                     #os.makedirs(f'out/sample{j}', exist_ok=True)\n",
    "#                     with open(f\"../.data/out/sample{i}-actual:{y[j]}-pred:{predicted[j]}_at_{confidences[j]*100:0.2f}%_confident.html\", 'w') as f:\n",
    "#                         f.write(s)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                results.append(Sample(\n",
    "                    target_layer=target_layer,\n",
    "                    words = words,\n",
    "                    sensitivity = regions[j, :],\n",
    "                ))\n",
    "        print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_words(words, sensitivites):\n",
    "    k_v_store = []\n",
    "    \n",
    "    key = ''\n",
    "    value = ''\n",
    "    acc = 0\n",
    "    \n",
    "    for rune, sensitivity in zip(results[0].words, sensitivites):\n",
    "        acc += sensitivity\n",
    "        if len(rune) > 5 and value == '':\n",
    "            key = rune\n",
    "\n",
    "        elif len(rune) <= 5:\n",
    "            value += rune\n",
    "\n",
    "        elif len(rune) > 5 and value:\n",
    "            k_v_store.append((rune, value, acc.item() / (len(value)+1)))\n",
    "            key = ''\n",
    "            value = ''\n",
    "            acc = 0\n",
    "\n",
    "        else:\n",
    "            k_v_store.append((key, value, acc.item() / (len(value)+1)))\n",
    "            key = ''\n",
    "            value = ''\n",
    "            acc = 0\n",
    "            \n",
    "    return k_v_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_k_v: Dict[Tuple[str, str], List[float]] = {}\n",
    "num_entries = 0\n",
    "for result in tqdm(results):\n",
    "    sensitivities = sorted(parse_words(result.words, result.sensitivity), key= lambda tup: tup[-1], reverse=True)\n",
    "    \n",
    "    for (k, v, s) in sensitivities:\n",
    "        key = tuple((k, v))\n",
    "        most_k_v[key] = most_k_v.get(key, []) \n",
    "        \n",
    "        most_k_v[key].append(s)\n",
    "        num_entries += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_most_k_v = {}\n",
    "for k, v in most_k_v.items():\n",
    "    results_most_k_v[k] = np.array(most_k_v[k]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in sorted(results_most_k_v.items(), key= lambda tup: tup[-1], reverse=True)[:30]:\n",
    "    print(f\"({thing[0][0]}) = \\\"{thing[0][1]}\\\" \\t\\twas present {(thing[1]/num_entries)*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(('script_item_active.med_active_ingr', '1'), 1552.4205774287411)\n",
    "(('observation_active.observation_value', 'O2SAT'), 1101.7255629818114)\n",
    "(('encounter_reason_active.reason', 'RESULTS DISCUSSED'), 1037.018368516322)\n",
    "(('script_item_active.dose', '20mg'), 1018.9674180358202)\n",
    "(('script_item_active.strength', '3'), 1013.4701462443155)\n",
    "(('script_item_active.repeats', '25'), 1009.5255802461905)\n",
    "(('script_item_active.quantity', 'PIROXICAM'), 999.4156962497607)\n",
    "(('script_item_active.med_name', 'PIROXICAM'), 983.5153352558156)\n",
    "(('script_item_active.frequency', '1'), 976.3854911817947)\n",
    "(('encounter_reason_active.reason', 'ELEVATED PSA'), 939.2458728121164)\n",
    "(('observation_active.observation_value', 'PULSE'), 930.0018636126786)\n",
    "(('script_item_active.repeats', '1'), 791.5672112685506)\n",
    "(('observation_active.observation_name', '98'), 791.0192584013294)\n",
    "(('script_item_active.strength', '0'), 726.3520064711704)\n",
    "(('script_item_active.dose', '0.1%'), 619.3899866717061)\n",
    "(('script_item_active.quantity', 'ADVANTAN'), 610.9945505812882)\n",
    "(('script_item_active.med_name', 'METHYLPREDNISOLONE ACEPONATE'), 594.2070541277367)\n",
    "(('script_item_active.frequency', 'Topical'), 572.4618293443946)\n",
    "(('script_item_active.strength', '2'), 521.2934443520304)\n",
    "(('script_item_active.med_active_ingr', '8'), 440.6237507160537)\n",
    "(('script_item_active.repeats', '20'), 343.97818737079285)\n",
    "(('script_item_active.quantity', 'TRAMADOL'), 341.27871390079196)\n",
    "(('encounter_reason_active.reason', 'ADVICE AND LISTENING'), 340.3012212191011)\n",
    "(('script_item_active.frequency', '50-100mg'), 336.9993179654589)\n",
    "(('script_item_active.med_name', 'TRAMADOL HYDROCHLORIDE'), 336.2271525825771)\n",
    "(('observation_active.observation_value', 'TEMP'), 319.9377679076771)\n",
    "(('encounter_reason_active.reason', 'DERMATITIS'), 280.79228743576834)\n",
    "(('encounter_reason_active.reason', 'COUNSELLING'), 203.64822078745178)\n",
    "(('encounter_reason_active.reason', 'WOUND REVIEW'), 192.2942505436592)\n",
    "(('immunisation_active.vaccine_name', 'FLUQUADRI'), 182.15081978951784)\n",
    "â€‹\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
