{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.experiments import get_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment = get_experiment('resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import glob\n",
    "import pytorch_lightning.metrics as pl_metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_results.pickle', 'rb') as handle:\n",
    "    all_results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# weights_to_test = [weight for weight in glob.glob('../mlruns/2/**/artifacts/weights/*.ckpt') if weight not in all_results]\n",
    "\n",
    "# for model_path in weights_to_test:\n",
    "    \n",
    "#     trainer = pl.Trainer(gpus=1, precision=16)\n",
    "\n",
    "#     experiment = experiment.load_from_checkpoint(\n",
    "#     #     '../2_1635799472.ckpt',\n",
    "#         model_path,\n",
    "#         train_data_path='../.data/medical_data_train_with_metadata',\n",
    "#         val_data_path='../.data/medical_data_val_with_metadata',\n",
    "#         learning_rate= 0.003,\n",
    "#         batch_size= 512,\n",
    "#         sequence_length= 16384,\n",
    "#         metrics = {\n",
    "#             \"stats\": torchmetrics.StatScores(reduce='macro', num_classes=2 ),\n",
    "#             \"confusion\": torchmetrics.ConfusionMatrix(normalize='true', num_classes=2 ),\n",
    "#             \"auroc\": torchmetrics.AUROC(average='macro', num_classes=2 ),\n",
    "#             \"f1_score\": torchmetrics.F1(average='macro', num_classes=2 ),\n",
    "#             \"precision\": torchmetrics.Precision(average='macro', num_classes=2 ),\n",
    "#             \"recall\": torchmetrics.Recall(average='macro', num_classes=2 ),\n",
    "#             \"accuracy\": torchmetrics.Accuracy(average='macro', num_classes=2 ),\n",
    "#         },\n",
    "#         balanced_dataloaders=False,\n",
    "#         strict=False\n",
    "\n",
    "#     )\n",
    "    \n",
    "#     def test_step(batch, batch_idx) -> None:\n",
    "#         _, y, y_hat, loss, patient_id = experiment.step(batch, stage=\"test\")\n",
    "\n",
    "#         experiment.log(\n",
    "#             f\"test_loss\", loss.item()\n",
    "#         )\n",
    "\n",
    "#         y_prob = torch.softmax(y_hat, 1).cpu()\n",
    "#         y_hat = y_hat.cpu()\n",
    "#         y_pred = torch.argmax(y_hat, dim=1).cpu()\n",
    "#         y = y.cpu()\n",
    "\n",
    "#     #     experiment.results[\"confusion\"].append(confusion_matrix(y_true=y, y_pred=y_pred, normalize='true').ravel())\n",
    "#     #     experiment.results[\"precision\"].append(precision_score(y_true=y, y_pred=y_pred))\n",
    "#     #     experiment.results[\"recall\"].append(recall_score(y_true=y, y_pred=y_pred))\n",
    "#     #     experiment.results[\"accuracy\"].append(accuracy_score(y_true=y, y_pred=y_pred))\n",
    "\n",
    "#         experiment.results[\"make_auroc_chart\"].append({'y_true': y, 'y_prob': y_prob, 'y_pred': y_pred, 'patient_id': patient_id})\n",
    "\n",
    "#     experiment.results = defaultdict(list)\n",
    "#     experiment.test_step = test_step\n",
    "    \n",
    "#     test_results = trainer.test(\n",
    "#         experiment, datamodule=experiment.data_module\n",
    "#     )\n",
    "    \n",
    "#     all_results[model_path] = {\n",
    "#         'y_true': torch.stack([item for sublist in experiment.results['make_auroc_chart'] for item in sublist['y_true']]),\n",
    "#         'y_probas' : torch.stack([item for sublist in experiment.results['make_auroc_chart'] for item in sublist['y_prob']]),\n",
    "#         'patient_ids' : torch.stack([item for sublist in experiment.results['make_auroc_chart'] for item in sublist['patient_id']]),\n",
    "    \n",
    "#     }\n",
    "    \n",
    "# with open('all_results.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_model_results = []\n",
    "for model_key, results in all_results.items():\n",
    "    model_key_id = model_key.split('/')[3]\n",
    "    \n",
    "    \n",
    "    target = results['y_true']\n",
    "    preds = torch.argmax(results['y_probas'], dim=1)\n",
    "    patient_ids = results['patient_ids']\n",
    "    \n",
    "#     print(model_key_id)\n",
    "#     print(metrics.classification_report(target, preds))\n",
    "\n",
    "    auc_fpr, auc_tpr, auc_thresholds = metrics.roc_curve(target, results['y_probas'][:, -1])\n",
    "    \n",
    "    optimal_idx = np.argmax(auc_tpr - auc_fpr)\n",
    "    optimal_threshold = auc_thresholds[optimal_idx]\n",
    "    print(optimal_threshold)\n",
    "    \n",
    "    def cutoff_youdens_j(fpr,tpr,thresholds):\n",
    "        j_scores = tpr-fpr\n",
    "        j_ordered = sorted(zip(j_scores,thresholds))\n",
    "        return j_ordered[-1][1]\n",
    "\n",
    "    print(cutoff_youdens_j(auc_fpr, auc_tpr, auc_thresholds))\n",
    "    \n",
    "    data = pd.DataFrame(\n",
    "        zip(target.numpy(), results['y_probas'][:, -1].numpy(), patient_ids.cpu().numpy()), \n",
    "        columns = ['true_label', f'confidence_in_being_hypertensive', 'patient_id']\n",
    "    )\n",
    "    data['model_no'] = f'{model_key_id}'\n",
    "    \n",
    "    all_model_results.append(data)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "lw = 2\n",
    "best_threshold = metrics.auc(auc_fpr, auc_tpr)\n",
    "plt.plot(auc_fpr, auc_tpr, color='darkorange', lw=lw, label=f'ROC curve (area = {best_threshold:0.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(len(auc_tpr)) # index for df\n",
    "roc = pd.DataFrame(\n",
    "    {\n",
    "        'fpr' : pd.Series(auc_fpr, index=i),\n",
    "        'tpr' : pd.Series(auc_tpr, index = i), \n",
    "        '1-fpr' : pd.Series(1-auc_fpr, index = i), \n",
    "        'tf' : pd.Series(auc_tpr - (1-auc_fpr), index = i), \n",
    "        'thresholds' : pd.Series(auc_thresholds, index = i)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "_ = plt.plot(roc['tpr'], label='sensitivity/tpr')\n",
    "_ = plt.plot(roc['1-fpr'], color = 'red', label='specificity/1-fpr')\n",
    "_ = plt.xlabel('1-False Positive Rate')\n",
    "_ = plt.ylabel('True Positive Rate')\n",
    "_ = plt.title('Receiver operating characteristic')\n",
    "_ = ax.set_xticklabels([])\n",
    "_ = plt.legend()\n",
    "\n",
    "# print(roc.iloc[(roc.tf).abs().argsort()[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(auc_tpr * (1-auc_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (auc_thresholds[ix], gmeans[ix]))\n",
    "\n",
    "tuned_fpr = auc_fpr[ix] \n",
    "tuned_tpr = auc_tpr[ix]\n",
    "tuned_threshold = auc_thresholds[ix]\n",
    "\n",
    "tuned_sensitivity = tuned_tpr / (tuned_tpr + tuned_fpr)\n",
    "tuned_tnr = 1-tuned_fpr\n",
    "tuned_specificity = tuned_tnr/(tuned_tnr+tuned_fpr)\n",
    "tuned_fnr = 1 - tuned_tpr\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "non_hypertensive_pateints = {125262}\n",
    "hypertensive_patients = {11714}\n",
    "\n",
    "TP = {tuned_tpr:0.3f},   FP = {tuned_fpr:0.3f}\n",
    "FN = {tuned_fnr:0.3f},  TN = {tuned_tnr:0.3f}\n",
    "\n",
    "Of those patients with hypertension, \n",
    "  number of patients classified as hypertensive = tp = {tuned_tpr:0.3f}\n",
    "  \n",
    "Of those patients without hypertension, \n",
    "  number of patients classified as not hypertensive = tn = {tuned_tnr:0.3f}\n",
    "\n",
    "sensitivity = tp/(tp+fn) = {tuned_sensitivity*100:0.2f}%\n",
    "\n",
    "specificity = tn/(tn+fp) = {tuned_specificity*100:0.2f}%\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually defining the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_value = 0.5\n",
    "a_list = auc_thresholds.tolist()\n",
    "absolute_difference_function = lambda list_value : abs(list_value - given_value)\n",
    "\n",
    "closest_value = min(a_list, key=absolute_difference_function)\n",
    "closest_index = a_list.index(closest_value)\n",
    "print(closest_value, closest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tpr = auc_tpr[closest_index]\n",
    "test_sensitivity = test_tpr\n",
    "test_sensitivity\n",
    "f\"sensitivity = {test_sensitivity*100:0.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fpr = auc_fpr[closest_index] \n",
    "test_specificity = 1-test_fpr\n",
    "f\"specificity = {test_specificity*100:0.2f}%\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
